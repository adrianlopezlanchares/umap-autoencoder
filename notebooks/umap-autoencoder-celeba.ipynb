{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b805471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from utils.data import (\n",
    "    download_celeba_data,\n",
    "    get_celeba_image_np_arrays,\n",
    "    UMAPImageDataset,\n",
    ")\n",
    "from utils.autoencoder import CelebAAutoencoder\n",
    "from utils.loss import UMAPAutoencoderLoss\n",
    "from utils.train_functions import (\n",
    "    train_autoencoder,\n",
    "    train_autoencoder_with_umap,\n",
    "    test_autoencoder_reconstruction,\n",
    "    test_autoencoder_umap_embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1b14b2",
   "metadata": {},
   "source": [
    "# Load CelebA images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "379f1bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is already downloaded. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "download_celeba_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c857481c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images...\n",
      "20259 images loaded          \n"
     ]
    }
   ],
   "source": [
    "# WARNING!! The dataset is very large (1.7GB), so this will take a very long time\n",
    "# Processing 10% of images took me 1 hour to load and compute UMAP embeddings (@ umap embedding_dim=2).\n",
    "train_images, test_images = get_celeba_image_np_arrays(process_size=0.1)\n",
    "full_images = np.concatenate([train_images, test_images], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99e829b",
   "metadata": {},
   "source": [
    "# UMAP embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4142c431",
   "metadata": {},
   "source": [
    "## Create & Save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd2aad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMENTED OUT TO AVOID REGENERATING UMAP EMBEDDINGS\n",
    "# Remove comments to regenerate UMAP embeddings\n",
    "\n",
    "# embedding_dim = 1024\n",
    "\n",
    "# flat_full_images = full_images.reshape(full_images.shape[0], -1)\n",
    "# reducer = umap.UMAP(n_components=embedding_dim)\n",
    "# full_embeddings = reducer.fit_transform(flat_full_images)\n",
    "\n",
    "# train_embeddings = full_embeddings[: len(train_images)]\n",
    "# test_embeddings = full_embeddings[len(train_images) :]\n",
    "\n",
    "# # Save full embeddings to a file\n",
    "# project_dir = Path(\"..\").resolve()\n",
    "# embeddings_file = (\n",
    "#     project_dir\n",
    "#     / \"data\"\n",
    "#     / \"img_align_celeba\"\n",
    "#     / \"umap_embeddings\"\n",
    "#     / f\"embeddings_d{embedding_dim}_n{len(full_images)}.npy\"\n",
    "# )\n",
    "# embeddings_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "# np.save(embeddings_file, full_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb0efa3",
   "metadata": {},
   "source": [
    "## Load embeddings if previously computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07deab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 1024\n",
    "\n",
    "project_dir = Path(\"..\").resolve()\n",
    "data_dir = project_dir / \"data\" / \"img_align_celeba\" / \"umap_embeddings\"\n",
    "embeddings_file = data_dir / f\"embeddings_d{embedding_dim}_n{len(full_images)}.npy\"\n",
    "\n",
    "full_embeddings = np.load(embeddings_file)\n",
    "full_images = np.concatenate([train_images, test_images], axis=0)\n",
    "flat_full_images = full_images.reshape(full_images.shape[0], -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd8d6b4",
   "metadata": {},
   "source": [
    "# Train Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37f0ba2",
   "metadata": {},
   "source": [
    "## Train & test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52b57b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "flat_train_images = train_images.reshape(train_images.shape[0], -1)\n",
    "flat_test_images = test_images.reshape(test_images.shape[0], -1)\n",
    "\n",
    "train_embeddings = full_embeddings[: len(train_images)]\n",
    "test_embeddings = full_embeddings[len(train_images) :]\n",
    "\n",
    "flat_train_dataset = UMAPImageDataset(\n",
    "    images=flat_train_images, umap_embeddings=train_embeddings\n",
    ")\n",
    "flat_test_dataset = UMAPImageDataset(\n",
    "    images=flat_test_images, umap_embeddings=test_embeddings\n",
    ")\n",
    "\n",
    "flat_train_loader = torch.utils.data.DataLoader(\n",
    "    flat_train_dataset, batch_size=128, shuffle=False\n",
    ")\n",
    "flat_test_loader = torch.utils.data.DataLoader(\n",
    "    flat_test_dataset, batch_size=128, shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f772758e",
   "metadata": {},
   "source": [
    "## Train normal Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b83177",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_input_dim = flat_full_images.shape[1]\n",
    "normal_autoencoder = CelebAAutoencoder(\n",
    "    input_dim=autoencoder_input_dim, embedding_dim=embedding_dim\n",
    ")\n",
    "normal_criterion = torch.nn.MSELoss()\n",
    "normal_optimizer = torch.optim.Adam(normal_autoencoder.parameters(), lr=1e-3)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else device)\n",
    "\n",
    "normal_num_epochs = 200\n",
    "\n",
    "train_autoencoder(\n",
    "    model=normal_autoencoder,\n",
    "    dataloader=flat_train_loader,\n",
    "    criterion=normal_criterion,\n",
    "    optimizer=normal_optimizer,\n",
    "    num_epochs=normal_num_epochs,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71273ad0",
   "metadata": {},
   "source": [
    "## Train Autoencoder with UMAP as teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eac6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_input_dim = flat_full_images.shape[1]\n",
    "umap_autoencoder = CelebAAutoencoder(\n",
    "    input_dim=autoencoder_input_dim, embedding_dim=embedding_dim\n",
    ")\n",
    "\n",
    "umap_criterion = UMAPAutoencoderLoss(reconstruction_weight=0.5, umap_weight=0.5)\n",
    "umap_optimizer = torch.optim.Adam(umap_autoencoder.parameters(), lr=1e-3)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else device)\n",
    "\n",
    "umap_num_epochs = 100\n",
    "\n",
    "train_autoencoder_with_umap(\n",
    "    model=umap_autoencoder,\n",
    "    dataloader=flat_train_loader,\n",
    "    criterion=umap_criterion,\n",
    "    optimizer=umap_optimizer,\n",
    "    num_epochs=umap_num_epochs,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0300bded",
   "metadata": {},
   "source": [
    "# Compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f670686",
   "metadata": {},
   "source": [
    "## Compare image results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd4d6e",
   "metadata": {},
   "source": [
    "### Plot normal Autoencoder results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e9ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot example image and reconstructions\n",
    "normal_autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    sample_images, _ = next(iter(flat_test_loader))\n",
    "    sample_images = sample_images.to(device).float()\n",
    "    reconstructed = normal_autoencoder(sample_images).cpu().numpy()\n",
    "    sample_images = sample_images.cpu().numpy()\n",
    "    print(sample_images.shape)\n",
    "    n = 10\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        # Original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(sample_images[i].reshape(64, 64, 3))\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Reconstructed\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(reconstructed[i].reshape(64, 64, 3))\n",
    "        plt.title(\"Reconstructed\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d8c83",
   "metadata": {},
   "source": [
    "### Plot UMAP Autoecoder results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd62018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot example image and reconstructions\n",
    "umap_autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    sample_images, _ = next(iter(flat_test_loader))\n",
    "    sample_images = sample_images.to(device).float()\n",
    "    reconstructed = umap_autoencoder(sample_images).cpu().numpy()\n",
    "    sample_images = sample_images.cpu().numpy()\n",
    "    n = 10\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        # Original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(sample_images[i].reshape(64, 64, 3), cmap=\"gray\")\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Reconstructed\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(reconstructed[i].reshape(64, 64, 3), cmap=\"gray\")\n",
    "        plt.title(\"Reconstructed\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c124033d",
   "metadata": {},
   "source": [
    "## Compare Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e8add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "normal_reconstruction_loss = test_autoencoder_reconstruction(\n",
    "    normal_autoencoder, flat_test_loader, device\n",
    ")\n",
    "umap_reconstruction_loss = test_autoencoder_reconstruction(\n",
    "    umap_autoencoder, flat_test_loader, device\n",
    ")\n",
    "\n",
    "print(f\"Normal Autoencoder Reconstruction Loss: {normal_reconstruction_loss:.4f}\")\n",
    "print(f\"UMAP Autoencoder Reconstruction Loss: {umap_reconstruction_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb5e673",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_umap_embedding_loss = test_autoencoder_umap_embedding(\n",
    "    normal_autoencoder, flat_test_loader, device\n",
    ")\n",
    "umap_umap_embedding_loss = test_autoencoder_umap_embedding(\n",
    "    umap_autoencoder, flat_test_loader, device\n",
    ")\n",
    "\n",
    "print(f\"Normal Autoencoder UMAP Embedding Loss: {normal_umap_embedding_loss:.4f}\")\n",
    "print(f\"UMAP Autoencoder UMAP Embedding Loss: {umap_umap_embedding_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf384ace",
   "metadata": {},
   "source": [
    "## Compare latent spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53cb329",
   "metadata": {},
   "source": [
    "### Normal Autoencoder latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2205c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_autoencoder.eval()\n",
    "all_latents = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in flat_test_loader:\n",
    "        images = images.to(device).float()\n",
    "        latents = normal_autoencoder.encoder(images).cpu().numpy()\n",
    "        all_latents.append(latents)\n",
    "all_latents = np.concatenate(all_latents, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax[0].scatter(\n",
    "    all_latents[:, 0],\n",
    "    all_latents[:, 1],\n",
    "    s=1,\n",
    "    alpha=0.5,\n",
    "    label=\"Normal Autoencoder\",\n",
    ")\n",
    "\n",
    "ax[1].scatter(\n",
    "    test_embeddings[:, 0],\n",
    "    test_embeddings[:, 1],\n",
    "    s=1,\n",
    "    alpha=0.5,\n",
    "    label=\"UMAP Embeddings\",\n",
    ")\n",
    "\n",
    "ax[0].set_title(\"Latent Space of Normal Autoencoder\")\n",
    "ax[0].set_xlabel(\"Latent Dimension 1\")\n",
    "ax[0].set_ylabel(\"Latent Dimension 2\")\n",
    "ax[1].set_title(\"UMAP Embeddings\")\n",
    "ax[1].set_xlabel(\"Latent Dimension 1\")\n",
    "ax[1].set_ylabel(\"Latent Dimension 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf95703",
   "metadata": {},
   "source": [
    "### UMAP Autoencoder latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aca713",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_autoencoder.eval()\n",
    "all_latents = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in flat_test_loader:\n",
    "        images = images.to(device).float()\n",
    "        latents = umap_autoencoder.encoder(images).cpu().numpy()\n",
    "        all_latents.append(latents)\n",
    "all_latents = np.concatenate(all_latents, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax[0].scatter(\n",
    "    all_latents[:, 0],\n",
    "    all_latents[:, 1],\n",
    "    s=1,\n",
    "    alpha=0.5,\n",
    "    label=\"Normal Autoencoder\",\n",
    ")\n",
    "\n",
    "ax[1].scatter(\n",
    "    test_embeddings[:, 0],\n",
    "    test_embeddings[:, 1],\n",
    "    s=1,\n",
    "    alpha=0.5,\n",
    "    label=\"UMAP Embeddings\",\n",
    ")\n",
    "\n",
    "ax[0].set_title(\"Latent Space of UMAP Autoencoder\")\n",
    "ax[0].set_xlabel(\"Latent Dimension 1\")\n",
    "ax[0].set_ylabel(\"Latent Dimension 2\")\n",
    "ax[1].set_title(\"UMAP Embeddings\")\n",
    "ax[1].set_xlabel(\"Latent Dimension 1\")\n",
    "ax[1].set_ylabel(\"Latent Dimension 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f8d50",
   "metadata": {},
   "source": [
    "# Recreate images from UMAP embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14951f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass all test UMAP embeddings through the UMAP decoder\n",
    "reconstructed_test_images = (\n",
    "    umap_autoencoder.decoder(\n",
    "        torch.tensor(test_embeddings, dtype=torch.float32).to(device)\n",
    "    )\n",
    "    .cpu()\n",
    "    .detach()\n",
    "    .numpy()\n",
    ")\n",
    "\n",
    "# Plot some reconstructed images from UMAP embeddings\n",
    "n = 20\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(test_images[i].reshape(64, 64, 3))\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Reconstructed from UMAP embedding\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(reconstructed_test_images[i].reshape(64, 64, 3))\n",
    "    plt.title(\"Reconstructed from UMAP Embedding\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umap-autoencoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
